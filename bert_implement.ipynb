{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8805cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e892b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_heads, num_layers, dropout, max_len = 1000):\n",
    "        token_vec_size = num_hiddens\n",
    "        super(BERTModel, self).__init__()\n",
    "\n",
    "        # 1. Embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, token_vec_size) \n",
    "        # \"Kati ota vectors store garne\" -> vocab_size (e.g. 30,000)\n",
    "        # \"each vector kati dim ko\" -> token_vec_size (e.g. 768)\n",
    "        # This creates a matrix of size 30,000 x 768. It initializes them randomly.\n",
    "\n",
    "        self.segment_embedding = nn.Embedding(2, token_vec_size) # 2 segments ofc setence A ki sentence B\n",
    "        # Embedding for sentence A and for sentence B each of 768 dim. i.e matrix of 2 x 768 vector\n",
    "        # Input Batch Shape: (Batch_Size, Max_Len, Token_Vec_Size) thus the 1, needs this for broadcasting as we do batch processing\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, token_vec_size)) # This is a learnable parameter\n",
    "        # It shape is (1,1000, 768), i,e 1000 vectors each of 768 size, each for 1 position, i.e. 1 vector for each position 0, 1, 2 ... 999\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 2. Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=token_vec_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=token_vec_size*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 3. Task Heads\n",
    "        # A. Next Sentence Prediction (NSP) Head\n",
    "        self.hidden = nn.Sequential(nn.Linear(token_vec_size, token_vec_size),nn.Tanh())\n",
    "        self.nsp = nn.Linear(token_vec_size, 2)\n",
    "\n",
    "        # B. Masked Language Modeling (MLM) Head\n",
    "        self.mlm=nn.Sequential(\n",
    "            nn.Linear(token_vec_size, token_vec_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(token_vec_size),\n",
    "            # This is simply just normalization, sepcifically standarization lol\n",
    "            nn.Linear(token_vec_size,vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, segments, pred_positions=None):\n",
    "        # 1. Input Embedding\n",
    "        X = self.token_embeddings(tokens)+ self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:,:X.shape[1],:]\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        # 2. Encoder Pass\n",
    "        encoded_X = self.encoder(X)\n",
    "\n",
    "        # 3. NSP Output\n",
    "        # Token 0 ([CLS]): Is the designated \"Summary Token\"\n",
    "        '''Because of the Self-Attention mechanism, every token looks at every other token. During pre-training, \n",
    "        the model is explicitly taught (via backpropagation) to put a global representation of the entire sentence pair into the [CLS] vector.'''\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "        # 4. MLM Output\n",
    "        if pred_positions is not None:\n",
    "            # We only want to predict at the masked positions\n",
    "            # pred_positions shape: (batch_size, num_preds)\n",
    "            batch_size = X.shape[0]\n",
    "            num_pred_positions = pred_positions.shape[1] # [[1,2],[2,3][4,5]] shape(3, 2)\n",
    "            \n",
    "            # Create batch indices to select specific vectors\n",
    "            batch_idx = torch.arange(0, batch_size).unsqueeze(1).repeat(1, num_pred_positions)\n",
    "            '''\n",
    "                [B]  →  [0, 1, 2, ..., B-1]\n",
    "\n",
    "                    =>\n",
    "\n",
    "                [B] → [B, 1]\n",
    "\n",
    "                [[0],\n",
    "                [1],\n",
    "                [2],\n",
    "                ...\n",
    "                [B-1]]\n",
    "\n",
    "\n",
    "                    =>\n",
    "\n",
    "                [B, 1] → [B, N]\n",
    "\n",
    "                [[0, 0, 0, ..., 0],\n",
    "                [1, 1, 1, ..., 1],\n",
    "                [2, 2, 2, ..., 2],\n",
    "                ...\n",
    "                [B-1, B-1, B-1, ..., B-1]]\n",
    "\n",
    "\n",
    "            '''\n",
    "            \n",
    "            # Gather the vectors at the masked positions\n",
    "            masked_X = encoded_X[batch_idx, pred_positions]\n",
    "            '''\n",
    "                encoded_X.shape      # (2, 5, 3)\n",
    "                pred_positions = [[1, 3],\n",
    "                                [0, 4]]\n",
    "\n",
    "                batch_idx = [[0, 0],\n",
    "                            [1, 1]]\n",
    "\n",
    "                            \n",
    "                masked_X[0, 0] = encoded_X[0, 1]\n",
    "                masked_X[0, 1] = encoded_X[0, 3]\n",
    "                masked_X[1, 0] = encoded_X[1, 0]\n",
    "                masked_X[1, 1] = encoded_X[1, 4]\n",
    "\n",
    "            '''\n",
    "            masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "            \n",
    "            mlm_Y_hat = self.mlm(masked_X)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        \n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d0e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_fake_data(vocab_size, batch_size, seq_len, num_mlm_preds):\n",
    "    \n",
    "    # Random tokens indices\n",
    "    tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Random segments (0s and 1s)\n",
    "    segments = torch.randint(0, 2, (batch_size, seq_len))\n",
    "    \n",
    "    # Random positions to mask (e.g., 15% of sequence)\n",
    "    pred_positions = torch.randint(0, seq_len, (batch_size, num_mlm_preds))\n",
    "    \n",
    "    # Fake Ground Truth Labels\n",
    "    # MLM Labels: The true words at the masked positions\n",
    "    mlm_labels = torch.randint(0, vocab_size, (batch_size, num_mlm_preds))\n",
    "    \n",
    "    # MLM Weights: 1.0 for real masks, 0.0 for padding (all 1s here for simplicity)\n",
    "    mlm_weights = torch.ones((batch_size, num_mlm_preds))\n",
    "    \n",
    "    # NSP Labels: 0 (True Next) or 1 (Random Next)\n",
    "    nsp_labels = torch.randint(0, 2, (batch_size,))\n",
    "    \n",
    "    return tokens, segments, pred_positions, mlm_labels, mlm_weights, nsp_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efeb8bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2, 2, 1, 2, 2],\n",
       "         [1, 2, 0, 0, 1],\n",
       "         [0, 1, 0, 2, 0]]),\n",
       " tensor([[1, 0, 0, 1, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0]]),\n",
       " tensor([[3, 2],\n",
       "         [0, 2],\n",
       "         [4, 3]]),\n",
       " tensor([[1, 2],\n",
       "         [0, 1],\n",
       "         [2, 1]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([0, 1, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fake_data(3,3,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595bf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VOCAB_SIZE = 10000\n",
    "NUM_HIDDENS = 128\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 32\n",
    "NUM_MLM_PREDS = 5 # Approx 15% of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8de70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Simulation...\n",
      "Step 10: MLM Loss: 9.3282, NSP Loss: 0.8525\n",
      "Step 20: MLM Loss: 9.3183, NSP Loss: 0.6907\n",
      "Step 30: MLM Loss: 9.4742, NSP Loss: 0.6678\n",
      "Step 40: MLM Loss: 9.2611, NSP Loss: 0.7680\n",
      "Step 50: MLM Loss: 9.6456, NSP Loss: 0.6863\n"
     ]
    }
   ],
   "source": [
    "def train_bert_custom():\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    net = BERTModel(VOCAB_SIZE, NUM_HIDDENS, NUM_HEADS, NUM_LAYERS, DROPOUT)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Starting Training Simulation...\")\n",
    "    \n",
    "    # 2. Training Loop (Simulating 50 steps)\n",
    "    for step in range(50):\n",
    "        # Generate a batch of fake data\n",
    "        tokens, segments, pred_positions, mlm_labels, mlm_weights, nsp_labels = \\\n",
    "            generate_fake_data(VOCAB_SIZE, BATCH_SIZE, SEQ_LEN, NUM_MLM_PREDS)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        _, mlm_Y_hat, nsp_Y_hat = net.forward(tokens, segments, pred_positions)\n",
    "        \n",
    "        # --- Calculate Losses ---\n",
    "        \n",
    "        # A. MLM Loss\n",
    "        # Flatten predictions to (batch * num_preds, vocab)\n",
    "        mlm_l = loss_fn(mlm_Y_hat.reshape(-1, VOCAB_SIZE), mlm_labels.reshape(-1))\n",
    "        # Multiply by weights (to ignore padding if we had it)\n",
    "        mlm_l = mlm_l * mlm_weights.reshape(-1)\n",
    "        mlm_l = mlm_l.sum() / (mlm_weights.sum() + 1e-8)\n",
    "        \n",
    "        # B. NSP Loss\n",
    "        nsp_l = loss_fn(nsp_Y_hat, nsp_labels)\n",
    "        nsp_l = nsp_l.mean()\n",
    "        \n",
    "        # Total Loss\n",
    "        total_loss = mlm_l + nsp_l\n",
    "        \n",
    "        # Backward Pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"Step {step+1}: MLM Loss: {mlm_l.item():.4f}, NSP Loss: {nsp_l.item():.4f}\")\n",
    "\n",
    "# Run the training\n",
    "train_bert_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b178f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context Sensitivity Check:\n",
      "Crane Vector 1 (Flying): tensor([0.2405, 1.3969, 0.4458])\n",
      "Crane Vector 2 (Driver): tensor([0.2401, 1.5742, 0.3580])\n",
      "Euclidean Distance between vectors: 1.7053\n",
      "SUCCESS: The vectors are different! The model is context-sensitive.\n"
     ]
    }
   ],
   "source": [
    "def check_context_sensitivity():\n",
    "    net = BERTModel(VOCAB_SIZE, NUM_HIDDENS, NUM_HEADS, NUM_LAYERS, DROPOUT)\n",
    "    net.eval() # Set to evaluation mode\n",
    "    \n",
    "    # Sentence 1: \"A crane is flying\"\n",
    "    # Let's map these to fake IDs for demonstration\n",
    "    # <cls>=0, a=5, crane=10, is=11, flying=12, <sep>=2\n",
    "    tokens_1 = torch.tensor([[0, 5, 10, 11, 12, 2]])\n",
    "    segments_1 = torch.tensor([[0, 0, 0, 0, 0, 0]])\n",
    "    \n",
    "    # Sentence 2: \"A crane driver came\"\n",
    "    # <cls>=0, a=5, crane=10, driver=15, came=16, <sep>=2\n",
    "    tokens_2 = torch.tensor([[0, 5, 10, 15, 16, 2]])\n",
    "    segments_2 = torch.tensor([[0, 0, 0, 0, 0, 0]])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_1, _, _ = net(tokens_1, segments_1)\n",
    "        encoded_2, _, _ = net(tokens_2, segments_2)\n",
    "        \n",
    "    # Extract the vector for \"crane\" (Index 2 in both sentences)\n",
    "    crane_vec_1 = encoded_1[0, 2, :]\n",
    "    crane_vec_2 = encoded_2[0, 2, :]\n",
    "    \n",
    "    print(\"\\nContext Sensitivity Check:\")\n",
    "    print(f\"Crane Vector 1 (Flying): {crane_vec_1[:3]}\") # Print first 3 numbers\n",
    "    print(f\"Crane Vector 2 (Driver): {crane_vec_2[:3]}\")\n",
    "    \n",
    "    dist = torch.dist(crane_vec_1, crane_vec_2)\n",
    "    print(f\"Euclidean Distance between vectors: {dist.item():.4f}\")\n",
    "    if dist > 0.0:\n",
    "        print(\"SUCCESS: The vectors are different! The model is context-sensitive.\")\n",
    "\n",
    "check_context_sensitivity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
